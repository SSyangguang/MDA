# MDA
 Code for A Multi-scale Information Integration Framework for Infrared and Visible Image Fusion

# A Multi-scale Information Integration Framework for Infrared and Visible Image Fusion
Infrared and visible image fusion aims at generating a fused image containing the intensity and detail information of source images, and the key issue is effectively measuring and integrating the complementary information of multi-modality images from the same scene. Existing methods mostly adopt a simple weight in the loss function to decide the information retention of each modality rather than adaptively measuring complementary information for different image pairs. In this study, we propose a multi-scale dual attention (MDA) framework for infrared and visible image fusion, which is designed to measure and integrate complementary information in both structure and loss function at the image and patch level. In our method, the residual downsample block decomposes source images into three scales first. Then, dual attention fusion block integrates complementary information and generates a spatial and channel attention map at each scale for feature fusion. Finally, the output image is reconstructed by the residual reconstruction block. Loss function consists of image-level, feature-level and patch-level three parts, of which the calculation of the image-level and patch-level two parts are based on the weights generated by the complementary information measurement. Indeed, to constrain the pixel intensity distribution between the output and infrared image, a style loss is added. Our fusion results perform robust and informative across different scenarios. Qualitative and quantitative results on two datasets illustrate that our method is able to preserve both thermal radiation and detailed information from two modalities and achieve comparable results compared with the other state-of-the-art methods. Ablation experiments show the effectiveness of our information integration architecture and adaptively measure complementary information retention in the loss function.

# Environment
* Python 3.7
* Pytorch 1.8.0
* Opencv 4.5.0
* Numpy
* Matplotlib

# Training
Download the pre-trained vgg16 model from [VGG16](https://download.pytorch.org/models/vgg16-397923af.pth) and rename it vgg16.pth.  Placing the vgg16.pth in the same directory with model.py.

Download the KASIT dataset from [KAIST](https://github.com/SoonminHwang/rgbt-ped-detection).

Our work is trained on the KAIST dataset, so we write a function to read KAIST dataset.

Change line 23 of main.py to "train()".

`python main.py --kaist_path ./kaist-cvpr15/images`

The generated fusion model and log file locates at ./fusion_model/fusion_model.pth and ./fusion_log, respectively.

# Testing
Change line 23 of main.py to "test()".

You can change the test data path in option.py (suggested), or run the following command to add your test data path:

`python main.py --test_irimage_path ./21_pairs_tno/ir  --test_visimage_path ./21_pairs_tno/vis`

# Contact Me
If you have any questions about the paper or code, please email to me ssyanguang@gmail.com.
